{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karthik-mannava/Resnet50final/blob/main/resnet50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_bwHOkjvO5z",
        "outputId": "db0ecaff-4bf0-4b33-de0a-13f537852651"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdi17yxVvN6M",
        "outputId": "26b56d13-c6e9-4ff5-a3bd-532e1bebbd55"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchnet\n",
            "  Downloading torchnet-0.0.4.tar.gz (23 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from torchnet) (2.0.0+cu118)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from torchnet) (1.16.0)\n",
            "Collecting visdom\n",
            "  Downloading visdom-0.2.4.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch->torchnet) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch->torchnet) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->torchnet) (3.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch->torchnet) (3.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->torchnet) (4.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch->torchnet) (3.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->torchnet) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->torchnet) (3.25.2)\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.9/dist-packages (from visdom->torchnet) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from visdom->torchnet) (1.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from visdom->torchnet) (2.27.1)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.9/dist-packages (from visdom->torchnet) (6.2)\n",
            "Collecting jsonpatch\n",
            "  Downloading jsonpatch-1.32-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.9/dist-packages (from visdom->torchnet) (1.5.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from visdom->torchnet) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch->torchnet) (2.1.2)\n",
            "Collecting jsonpointer>=1.9\n",
            "  Downloading jsonpointer-2.3-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->visdom->torchnet) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->visdom->torchnet) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->visdom->torchnet) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->visdom->torchnet) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch->torchnet) (1.3.0)\n",
            "Building wheels for collected packages: torchnet, visdom\n",
            "  Building wheel for torchnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchnet: filename=torchnet-0.0.4-py3-none-any.whl size=29745 sha256=549efa540696016ca57d5bee6401746713bafb3582331da8621de54a09becf70\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/d2/1e/0c2519d1837089fe374a60ce10e120c555b9d77110a49f027f\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.2.4-py3-none-any.whl size=1408216 sha256=4f2f58daa5e9285c5d02f2654c3dae587214c836afd5dc1012be6afd265258a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/58/9e/14/30f7cc4dafdd4d602fb00ca33c6edd1424fc0f5df10a02e060\n",
            "Successfully built torchnet visdom\n",
            "Installing collected packages: jsonpointer, jsonpatch, visdom, torchnet\n",
            "Successfully installed jsonpatch-1.32 jsonpointer-2.3 torchnet-0.0.4 visdom-0.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import torchnet.meter.confusionmeter as cm"
      ],
      "metadata": {
        "id": "Vkol_yjnv8JS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation & test\n",
        "data_transforms = {\n",
        "    'Training': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'Validating': transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'Testing': transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'validation': transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.CenterCrop(224) ,\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485], [0.229])\n",
        "    ])\n",
        "}\n"
      ],
      "metadata": {
        "id": "Bjb2muUiv-Km"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/gdrive/MyDrive/Contrast_dataset_final-20230403T102414Z-001/Contrast_dataset_final'\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
        "                  for x in ['Testing','Training', 'Validating']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=16,\n",
        "                                             shuffle=True, num_workers=2)\n",
        "              for x in ['Testing','Training', 'Validating']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['Testing','Training', 'Validating']}\n",
        "class_names = image_datasets['Training'].classes\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "print(class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7DyKuq6v___",
        "outputId": "44a5ac8d-d0a8-449a-c472-4cd7e9fdb42f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "['glioma', 'meningioma', 'notumor', 'pituitory']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lists for graph generation\n",
        "epoch_counter_train = []\n",
        "epoch_counter_val = []\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "train_acc = []\n",
        "val_acc = []\n",
        "\n",
        "#Train the model\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs):\n",
        "    since = time.time()\n",
        "    print(since)\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    epoch_no = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['Training', 'Testing']:\n",
        "            if phase == 'Training':\n",
        "                # scheduler.step()\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'Training'):  \n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    probs = torch.exp(outputs) \n",
        "                    # print(probs)\n",
        "                    # print(_)\n",
        "                    # print(preds)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'Training':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics \n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            #For graph generation\n",
        "            if phase == \"Training\":\n",
        "                train_loss.append(running_loss/dataset_sizes[phase])\n",
        "                train_acc.append(running_corrects.double() / dataset_sizes[phase])\n",
        "                epoch_counter_train.append(epoch)\n",
        "            if phase == \"Testing\":\n",
        "                val_loss.append(running_loss/ dataset_sizes[phase])\n",
        "                val_acc.append(running_corrects.double() / dataset_sizes[phase]) \n",
        "                epoch_counter_val.append(epoch)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            #for printing        \n",
        "            if phase == \"Training\":    \n",
        "                epoch_loss = running_loss / dataset_sizes[phase]\n",
        "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            if phase == \"Testing\":    \n",
        "                epoch_loss = running_loss / dataset_sizes[phase]\n",
        "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            \n",
        "            \n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the best model\n",
        "            if phase == 'Testing' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                epoch_no = epoch\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f} in epoch {}'.format(best_acc, epoch_no))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    torch.save(model.state_dict(), '/content/gdrive/MyDrive/Contrast_dataset_final-20230403T102414Z-001/Model/resnetsoftmax2.pt')\n",
        "    return model"
      ],
      "metadata": {
        "id": "zGfmHAAWwB4X"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using a model pre-trained on ImageNet and replacing it's final linear layer\n",
        "\n",
        "#For resnet18\n",
        "model_ft = models.resnet50(pretrained=True)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "model_ft.fc = nn.Linear(num_ftrs, 4) \n",
        "\n",
        "#for VGG16_BN\n",
        "# model_ft = models.vgg16_bn(pretrained=True)\n",
        "# model_ft.classifier[6].out_features = 8\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Using Adam as the parameter optimizer\n",
        "optimizer_ft = optim.Adam(model_ft.parameters(), lr = 0.001, betas=(0.9, 0.999))\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)       \n",
        "\n"
      ],
      "metadata": {
        "id": "CzgoV3YhwES6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47d5fc16-c8c1-4267-975d-87a33d8fe536"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 230MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.version.cuda)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tdqaVQnwHX7",
        "outputId": "e6ed8fc2-c907-434e-c6a9-338f57339c11"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['glioma', 'meningioma', 'notumor', 'pituitory']"
      ],
      "metadata": {
        "id": "gMp6AT0HVtRp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load the state_dict\n",
        "state_dict = torch.load('/content/gdrive/MyDrive/Contrast_dataset_final-20230403T102414Z-001/Model/resnetsoftmax2.pt', map_location=torch.device('cpu'))\n",
        "\n",
        "# Initialize a new instance of the resnet50 model\n",
        "model_ft = models.resnet50(pretrained=True)\n",
        "\n",
        "# Replace the final fully connected layer with a new one\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "model_ft.fc = torch.nn.Linear(num_ftrs, 4)\n",
        "\n",
        "# Load the state_dict into the new model instance\n",
        "model_ft.load_state_dict(state_dict)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model_ft.eval()\n",
        "\n",
        "# Load the image and apply the required transformations\n",
        "image = Image.open('/content/gdrive/MyDrive/Contrast_dataset_final-20230403T102414Z-001/Contrast_dataset_final/Te-no_0025.jpg')\n",
        "\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "image = image_transforms(image)\n",
        "image = image.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Run inference on the image\n",
        "with torch.no_grad():\n",
        "    outputs = model_ft(image)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "# Print the predicted class label\n",
        "print('Predicted class:', classes[predicted.item()])\n",
        "probabilities = F.softmax(outputs, dim=1)\n",
        "probs_rounded = torch.round(probabilities * 1000) / 1000\n",
        "print(probs_rounded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVgOBIXTVC7q",
        "outputId": "6751f434-e61e-4dea-d063-cc4b53da686a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: notumor\n",
            "tensor([[0.0000, 0.0140, 0.9860, 0.0000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import/install Gradio \n",
        "try:\n",
        "    import gradio as gr\n",
        "except: \n",
        "    !pip -q install gradio\n",
        "    import gradio as gr\n",
        "    \n",
        "print(f\"Gradio version: {gr.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LaVfm19YmiU",
        "outputId": "d7f96369-a130-4578-b48d-d6d72a9e99d3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.3/75.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m140.5/140.5 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.7/129.7 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m286.2/286.2 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m269.3/269.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Gradio version: 3.27.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Load the state_dict\n",
        "state_dict = torch.load('/content/gdrive/MyDrive/Contrast_dataset_final-20230403T102414Z-001/Model/resnetsoftmax2.pt', map_location=torch.device('cpu'))\n",
        "\n",
        "# Initialize a new instance of the resnet50 model\n",
        "model_ft = models.resnet50(pretrained=True)\n",
        "\n",
        "# Replace the final fully connected layer with a new one\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "model_ft.fc = torch.nn.Linear(num_ftrs, 4)\n",
        "\n",
        "# Load the state_dict into the new model instance\n",
        "model_ft.load_state_dict(state_dict)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model_ft.eval()\n",
        "\n",
        "# Define the function to run inference on an image\n",
        "def predict_tumor(image):\n",
        "    # Load the image and apply the required transformations\n",
        "    image_transforms = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "    image = Image.fromarray(image.astype('uint8'), 'RGB')\n",
        "    image = image_transforms(image)\n",
        "    image = image.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    # Run inference on the image\n",
        "    with torch.no_grad():\n",
        "        outputs = model_ft(image)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    # Return the predicted class label\n",
        "    return classes[predicted.item()]\n",
        "\n",
        "# Define the Gradio interface\n",
        "inputs = gr.inputs.Image()\n",
        "outputs = gr.outputs.Label()\n",
        "interface = gr.Interface(fn=predict_tumor, inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Launch the Gradio interface\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 845
        },
        "id": "AvyorGTeYkq9",
        "outputId": "6382b693-2bb6-4bcb-8ceb-b49e87ffb200"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/gradio/inputs.py:257: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/gradio/deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
            "  warnings.warn(value)\n",
            "/usr/local/lib/python3.9/dist-packages/gradio/outputs.py:197: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/gradio/deprecation.py:40: UserWarning: The 'type' parameter has been deprecated. Use the Number component instead.\n",
            "  warnings.warn(value)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7860, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft.to('cpu')\n",
        "next(iter(model_ft.parameters())).device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NCk8AHrZDzh",
        "outputId": "42ee47aa-9b85-4acb-cd53-a50751b76533"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple, Dict\n",
        "import time\n",
        "\n",
        "def predict(img) -> Tuple[Dict, float]:\n",
        "  \n",
        "  \"\"\"Transforms and performs a prediction on img and returns prediction and time taken\"\"\"\n",
        "\n",
        "  # Start the timer\n",
        "  start_time = time.time()\n",
        "\n",
        "  # Transform the target image and add the batch dimension\n",
        "  img = image_transforms(img).unsqueeze(0)\n",
        "\n",
        "  # Put the model into evaluation mode and turn on inference\n",
        "  model_ft.eval()\n",
        "  with torch.inference_mode():\n",
        "    # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n",
        "    pred_probs = torch.round(F.softmax(model_ft(img), dim = 1) * 1000) / 1000\n",
        "  \n",
        "  # Create a prediction label and prediction probability dictionary for each prediction class\n",
        "  pred_labels_and_probs = {classes[i]: float(pred_probs[0][i]) for i in range(len(classes))}\n",
        "\n",
        "  # Calculate the prediction time\n",
        "  pred_time = round(time.time() - start_time, 5)\n",
        "\n",
        "  # Return the prediction dictionary and prediction time\n",
        "  return pred_labels_and_probs, pred_time"
      ],
      "metadata": {
        "id": "VspVKetLZj8k"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.open('/content/gdrive/MyDrive/Contrast_dataset_final-20230403T102414Z-001/Contrast_dataset_final/Te-no_0025.jpg')\n",
        "x, y = predict(image)\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvIVKztYdCzf",
        "outputId": "bd7d16a8-ecf1-4ad5-a74d-c5a3981109b2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'glioma': 0.0, 'meningioma': 0.014000000432133675, 'notumor': 0.9860000014305115, 'pituitory': 0.0}\n",
            "0.8108\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "# Get a list of all test image file paths\n",
        "#test_data_paths = list(Path('/content/gdrive/MyDrive/Brain_Tumor_Detection_3.0/Removing_extras_dataset/Validating').glob(\"*/*.jpg\"))\n",
        "test_data_paths=[\"/content/gdrive/MyDrive/Contrast_dataset_final-20230403T102414Z-001/Contrast_dataset_final/Te-gl_0036.jpg\",\n",
        "                 \"/content/gdrive/MyDrive/Contrast_dataset_final-20230403T102414Z-001/Contrast_dataset_final/Te-me_0015.jpg\",\n",
        "                 \"/content/gdrive/MyDrive/Contrast_dataset_final-20230403T102414Z-001/Contrast_dataset_final/Te-no_0025.jpg\"\n",
        "                 ]\n",
        "# Randomly select a test image \n",
        "random_image_path = random.sample(test_data_paths, k=1)[0]\n",
        "\n",
        "# Open the target image\n",
        "image = Image.open(random_image_path)\n",
        "print(f\"[INFO] Predicting on image at path: {random_image_path}\\n\")\n",
        "\n",
        "# Predict on the target image and print out the outputs\n",
        "pred_dict, pred_time = predict(img = image)\n",
        "print(f\"Prediction label and Probability dictionary: \\n{pred_dict}\")\n",
        "print(f\"Prediction time: {pred_time} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErILJVG5doKy",
        "outputId": "7b5ab316-39c6-4966-884d-d2974171d2f9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Predicting on image at path: /content/gdrive/MyDrive/Contrast_dataset_final-20230403T102414Z-001/Contrast_dataset_final/Te-me_0015.jpg\n",
            "\n",
            "Prediction label and Probability dictionary: \n",
            "{'glioma': 0.0, 'meningioma': 0.996999979019165, 'notumor': 0.003000000026077032, 'pituitory': 0.0}\n",
            "Prediction time: 0.19852 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of example inputs to out gradio app\n",
        "example_list = [[str(filepath)] for filepath in random.sample(test_data_paths, k=3)]\n",
        "example_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6tONT6QpZUH",
        "outputId": "a8aaf3b2-f5e8-48d3-b5f1-bca1a1d60025"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['/content/gdrive/MyDrive/Contrast_dataset_final-20230403T102414Z-001/Contrast_dataset_final/Te-me_0015.jpg'],\n",
              " ['/content/gdrive/MyDrive/Contrast_dataset_final-20230403T102414Z-001/Contrast_dataset_final/Te-no_0025.jpg'],\n",
              " ['/content/gdrive/MyDrive/Contrast_dataset_final-20230403T102414Z-001/Contrast_dataset_final/Te-gl_0036.jpg']]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Create title, description and article strings\n",
        "title = \"ðŸ§ Brain Tumor DetectorðŸ§ \"\n",
        "description = \"A Convolution Neural Network computer vision model to classify Brain Tumor Images\"\n",
        "article = \"Created at [Brain_Tumor_Detection](https://colab.research.google.com/drive/1yU-VMpPau6xdmq32fITNcV7r1QF8M-YS?usp=sharing).\"\\\n",
        "\"<h1>Overview</h1>\"\n",
        "\n",
        "# Create the Gradio demo\n",
        "demo = gr.Interface(fn=predict, # mapping function from input to output\n",
        "                    inputs=gr.Image(type=\"pil\"), # what are the inputs?\n",
        "                    outputs=[gr.Label(num_top_classes=4, label=\"Predictions\"), # what are the outputs?\n",
        "                             gr.Number(label=\"Prediction time (s)\")], # our fn has two outputs, therefore we have two outputs\n",
        "                    examples=example_list, \n",
        "                    title=title,\n",
        "                    description=description,\n",
        "                    article=article\n",
        "                    )\n",
        "\n",
        "# Launch the demo!\n",
        "demo.launch(debug=False, # print errors locally?\n",
        "            share=True) # generate a publically shareable URL?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "LhH-zswTqJjh",
        "outputId": "ec98eb7e-f4b7-4adc-db52-afdb003eda47"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://befdd40848cd5d3b43.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://befdd40848cd5d3b43.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}